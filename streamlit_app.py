import platform
import sys
import polars as pl
import duckdb
import streamlit as st
import os
import time
import logging
import io
import zipfile
from pathlib import Path
from typing import Dict, List, Optional, Any
import warnings
import json

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

EXCEL_ROW_LIMIT = 1_048_576

class HighVolumeAutoPartsCatalog:
    def __init__(self):
        self.data_dir = Path("./auto_parts_data")
        self.data_dir.mkdir(exist_ok=True)

        self.cloud_config = self.load_cloud_config()
        self.db_path = self.data_dir / "catalog.duckdb"
        self.conn = duckdb.connect(database=str(self.db_path))
        self.setup_database()

        # –ù–æ–≤–æ–µ: —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
        self.table_columns = {
            'oe_data': set(),
            'parts_data': set(),
            'cross_references': set(),
            'prices': set()
        }

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü
        self.load_table_structure()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª
        self.price_rules = self.load_price_rules()
        self.exclusion_rules = self.load_exclusion_rules()
        self.category_mapping = self.load_category_mapping()

        # UI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        st.set_page_config(
            page_title="üöó AutoParts Catalog",
            layout="wide",
            page_icon="üöó",
            initial_sidebar_state="expanded"
        )

    def load_cloud_config(self) -> Dict[str, Any]:
        config_path = self.data_dir / "cloud_config.json"
        default_config = {
            "enabled": False,
            "provider": "s3",
            "bucket": "",
            "region": "",
            "sync_interval": 3600,
            "last_sync": 0
        }
        if config_path.exists():
            try:
                return json.loads(config_path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è cloud_config.json: {e}")
                return default_config
        config_path.write_text(json.dumps(default_config, indent=2, ensure_ascii=False), encoding='utf-8')
        return default_config

    def save_cloud_config(self):
        config_path = self.data_dir / "cloud_config.json"
        self.cloud_config["last_sync"] = int(time.time())
        config_path.write_text(json.dumps(self.cloud_config, indent=2, ensure_ascii=False), encoding='utf-8')

    def load_price_rules(self) -> Dict[str, Any]:
        path = self.data_dir / "price_rules.json"
        default = {
            "global_markup": 0.2,
            "brand_markups": {},
            "min_price": 0.0,
            "max_price": 99999.0
        }
        if path.exists():
            try:
                return json.loads(path.read_text(encoding='utf-8'))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
                return default
        path.write_text(json.dumps(default, indent=2, ensure_ascii=False), encoding='utf-8')
        return default

    def save_price_rules(self):
        path = self.data_dir / "price_rules.json"
        path.write_text(json.dumps(self.price_rules, indent=2, ensure_ascii=False), encoding='utf-8')

    def load_exclusion_rules(self) -> List[str]:
        path = self.data_dir / "exclusion_rules.txt"
        if path.exists():
            try:
                return [line.strip() for line in path.read_text(encoding='utf-8').splitlines() if line.strip()]
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
                return []
        path.write_text("–ö—É–∑–æ–≤\n–°—Ç–µ–∫–ª–∞\n–ú–∞—Å–ª–∞", encoding='utf-8')
        return ["–ö—É–∑–æ–≤", "–°—Ç–µ–∫–ª–∞", "–ú–∞—Å–ª–∞"]

    def save_exclusion_rules(self):
        path = self.data_dir / "exclusion_rules.txt"
        path.write_text("\n".join(self.exclusion_rules), encoding='utf-8')

    def load_category_mapping(self) -> Dict[str, str]:
        path = self.data_dir / "category_mapping.txt"
        default = {
            "–†–∞–¥–∏–∞—Ç–æ—Ä": "–û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ",
            "–®–∞—Ä–æ–≤–∞—è –æ–ø–æ—Ä–∞": "–ü–æ–¥–≤–µ—Å–∫–∞",
            "–§–∏–ª—å—Ç—Ä –º–∞—Å–ª—è–Ω—ã–π": "–§–∏–ª—å—Ç—Ä—ã",
            "–¢–æ—Ä–º–æ–∑–Ω—ã–µ –∫–æ–ª–æ–¥–∫–∏": "–¢–æ—Ä–º–æ–∑–∞"
        }
        if path.exists():
            try:
                mapping = {}
                for line in path.read_text(encoding='utf-8').splitlines():
                    if line.strip() and "|" in line:
                        k, v = line.split("|", 1)
                        mapping[k.strip()] = v.strip()
                return mapping
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
                return default
        content = "\n".join([f"{k}|{v}" for k, v in default.items()])
        path.write_text(content, encoding='utf-8')
        return default

    def save_category_mapping(self):
        path = self.data_dir / "category_mapping.txt"
        content = "\n".join([f"{k}|{v}" for k, v in self.category_mapping.items()])
        path.write_text(content, encoding='utf-8')

    def setup_database(self):
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã, –µ—Å–ª–∏ –∏—Ö –µ—â–µ –Ω–µ—Ç
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS oe_data (
                oe_number_norm VARCHAR PRIMARY KEY,
                oe_number VARCHAR,
                name VARCHAR,
                applicability VARCHAR
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS parts_data (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                artikul VARCHAR,
                brand VARCHAR,
                multiplicity INTEGER,
                barcode VARCHAR,
                length DOUBLE,
                width DOUBLE,
                height DOUBLE,
                weight DOUBLE,
                image_url VARCHAR,
                dimensions_str VARCHAR,
                description VARCHAR,
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS cross_references (
                oe_number_norm VARCHAR,
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                PRIMARY KEY (oe_number_norm, artikul_norm, brand_norm)
            )
        """)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS prices (
                artikul_norm VARCHAR,
                brand_norm VARCHAR,
                price DOUBLE,
                currency VARCHAR DEFAULT 'RUB',
                PRIMARY KEY (artikul_norm, brand_norm)
            )
        """)
        self.create_indexes()

    def create_indexes(self):
        """–°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_oe_data_oe ON oe_data(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_parts_data_keys ON parts_data(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_oe ON cross_references(oe_number_norm)",
            "CREATE INDEX IF NOT EXISTS idx_cross_artikul ON cross_references(artikul_norm, brand_norm)",
            "CREATE INDEX IF NOT EXISTS idx_prices_keys ON prices(artikul_norm, brand_norm)"
        ]
        for sql in indexes:
            self.conn.execute(sql)

    def load_table_structure(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Ç–∞–±–ª–∏—Ü –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        for table in self.table_columns.keys():
            res = self.conn.execute(f"PRAGMA table_info({table})").fetchall()
            cols = {row[1] for row in res}
            self.table_columns[table] = cols

    def add_new_column(self, table_name: str, column_name: str, col_type: str = "VARCHAR"):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –≤ —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç"""
        if column_name not in self.table_columns[table_name]:
            try:
                self.conn.execute(f"ALTER TABLE {table_name} ADD COLUMN {column_name} {col_type}")
                self.table_columns[table_name].add(column_name)
                logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü {column_name} –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç–æ–ª–±—Ü–∞ {column_name} –≤ {table_name}: {e}")

    def update_table_structure_with_df(self, table_name: str, df: pl.DataFrame):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã –≤ –±–∞–∑–µ –ø–æ –Ω–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º –∏–∑ DataFrame"""
        existing_cols = self.table_columns.get(table_name, set())
        for col in df.columns:
            if col not in existing_cols:
                self.add_new_column(table_name, col)

    def read_and_prepare_file(self, file_path: str, file_type: str) -> pl.DataFrame:
        """–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞"""
        logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_type} ({file_path})")
        try:
            df = pl.read_excel(file_path, engine='calamine')
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return pl.DataFrame()

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        schemas = {
            'oe': ['oe_number', 'artikul', 'brand', 'name', 'applicability'],
            'barcode': ['brand', 'artikul', 'barcode', 'multiplicity'],
            'dimensions': ['artikul', 'brand', 'length', 'width', 'height', 'weight', 'dimensions_str'],
            'images': ['artikul', 'brand', 'image_url'],
            'cross': ['oe_number', 'artikul', 'brand'],
            'prices': ['artikul', 'brand', 'price', 'currency']
        }
        expected_cols = schemas.get(file_type, [])
        column_mapping = self.detect_columns(df.columns, expected_cols)

        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        if column_mapping:
            df = df.rename(column_mapping)
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ç–∏–ø–∞ {file_type}")

        # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        for col in ['artikul', 'brand', 'oe_number']:
            if col in df.columns:
                df = df.with_columns(self.clean_values(pl.col(col)).alias(col))
                df = df.with_columns(self.normalize_key(pl.col(col)).alias(f"{col}_norm"))

        df = df.unique()

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü
        table_name = self.get_table_name_by_type(file_type)
        if table_name:
            self.update_table_structure_with_df(table_name, df)

        return df

    def get_table_name_by_type(self, file_type: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞"""
        mapping = {
            'oe': 'oe_data',
            'cross': 'cross_references',
            'barcode': 'parts_data',
            'dimensions': 'parts_data',
            'images': 'parts_data',
            'prices': 'prices'
        }
        return mapping.get(file_type)

    def detect_columns(self, actual_columns: List[str], expected_columns: List[str]) -> Dict[str, str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        variants = {
            'oe_number': ['oe –Ω–æ–º–µ—Ä', 'oe', '–æe', '–Ω–æ–º–µ—Ä', 'code'],
            'artikul': ['–∞—Ä—Ç–∏–∫—É–ª', 'article', 'sku'],
            'brand': ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å'],
            'name': ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', '–æ–ø–∏—Å–∞–Ω–∏–µ'],
            'applicability': ['–ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'vehicle'],
            'barcode': ['—à—Ç—Ä–∏—Ö-–∫–æ–¥', 'barcode', 'ean'],
            'multiplicity': ['–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å —à—Ç', '–∫—Ä–∞—Ç–Ω–æ—Å—Ç—å', 'multiplicity'],
            'length': ['–¥–ª–∏–Ω–∞ (—Å–º)', '–¥–ª–∏–Ω–∞', 'length'],
            'width': ['—à–∏—Ä–∏–Ω–∞ (—Å–º)', '—à–∏—Ä–∏–Ω–∞', 'width'],
            'height': ['–≤—ã—Å–æ—Ç–∞ (—Å–º)', '–≤—ã—Å–æ—Ç–∞', 'height'],
            'weight': ['–≤–µ—Å (–∫–≥)', '–≤–µ—Å', 'weight'],
            'image_url': ['—Å—Å—ã–ª–∫–∞', 'url', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 'image'],
            'dimensions_str': ['–≤–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã', '—Ä–∞–∑–º–µ—Ä—ã', 'dimensions'],
            'price': ['—Ü–µ–Ω–∞', 'price'],
            'currency': ['–≤–∞–ª—é—Ç–∞', 'currency']
        }
        actual_lower = {col.lower(): col for col in actual_columns}
        mapping = {}
        for expected in expected_columns:
            for variant in variants.get(expected, [expected]):
                for key, orig in actual_lower.items():
                    if variant.lower() in key and orig not in mapping:
                        mapping[orig] = expected
                        break
        return mapping

    def clean_values(self, col: pl.Expr) -> pl.Expr:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫"""
        return pl.when(pl.col(col).is_null()).then("").otherwise(
            pl.col(col).cast(pl.Utf8).str.replace_all("'", "").str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]", "").str.strip()
        )

    def normalize_key(self, col: pl.Expr) -> pl.Expr:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π"""
        return pl.when(pl.col(col).is_null()).then("").otherwise(
            pl.col(col).cast(pl.Utf8).str.replace_all("'", "").str.replace_all(r"[^0-9A-Za-z–ê-–Ø–∞-—è–Å—ë`\\-\\s]", "").str.strip().str.to_lowercase()
        )

    def get_table_name_by_type(self, file_type: str) -> Optional[str]:
        mapping = {
            'oe': 'oe_data',
            'cross': 'cross_references',
            'barcode': 'parts_data',
            'dimensions': 'parts_data',
            'images': 'parts_data',
            'prices': 'prices'
        }
        return mapping.get(file_type)

    def upsert_data(self, table_name: str, df: pl.DataFrame, pk: List[str]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º —Ç–∞–±–ª–∏—Ü—ã"""
        if df.is_empty():
            return
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã
        self.update_table_structure_with_df(table_name, df)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DataFrame –≤ Arrow –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        self.conn.register("temp_df", df.to_arrow())
        pk_str = ", ".join(f'"{col}"' for col in pk)
        update_cols = [col for col in df.columns if col not in pk]
        if update_cols:
            set_clause = ", ".join([f'"{col}" = excluded."{col}"' for col in update_cols])
            sql = f"""
                INSERT INTO {table_name} SELECT * FROM temp_df
                ON CONFLICT ({pk_str}) DO UPDATE SET {set_clause}
            """
        else:
            sql = f"INSERT INTO {table_name} SELECT * FROM temp_df ON CONFLICT ({pk_str}) DO NOTHING"
        try:
            self.conn.execute(sql)
        finally:
            self.conn.unregister("temp_df")

    def update_table_structure_with_df(self, table_name: str, df: pl.DataFrame):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü—ã –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        existing_cols = self.table_columns.get(table_name, set())
        for col in df.columns:
            if col not in existing_cols:
                self.add_new_column(table_name, col, col_type="VARCHAR")
                existing_cols.add(col)

    def add_new_column(self, table_name: str, column_name: str, col_type: str = "VARCHAR"):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞"""
        try:
            self.conn.execute(f"ALTER TABLE {table_name} ADD COLUMN {column_name} {col_type}")
            self.table_columns[table_name].add(column_name)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç–æ–ª–±—Ü–∞ {column_name} –≤ {table_name}: {e}")

    def build_export_query(self, selected_columns=None, include_prices=True, apply_markup=True) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ SQL-–∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞, —Å —É—á–µ—Ç–æ–º –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        cols = selected_columns or [
            "–ê—Ä—Ç–∏–∫—É–ª", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å", "–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å", "–®—Ç—Ä–∏—Ö-–∫–æ–¥", "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–∞–Ω–∞–ª–æ–≥–∏"
        ]
        # –°–æ–±–∏—Ä–∞–µ–º SELECT
        select_parts = []
        select_parts.append("p.artikul AS \"–ê—Ä—Ç–∏–∫—É–ª\"")
        select_parts.append("p.brand AS \"–ë—Ä–µ–Ω–¥\"")
        select_parts.append("COALESCE(od.name, '–ù–µ —É–∫–∞–∑–∞–Ω–æ') AS \"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ\"")
        select_parts.append("COALESCE(od.applicability, '–î–ª—è –≤—Å–µ—Ö') AS \"–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å\"")
        select_parts.append("p.multiplicity AS \"–ö—Ä–∞—Ç–Ω–æ—Å—Ç—å\"")
        select_parts.append("p.barcode AS \"–®—Ç—Ä–∏—Ö-–∫–æ–¥\"")
        select_parts.append("p.image_url AS \"–°—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\"")
        select_parts.append("STRING_AGG(DISTINCT cr2.artikul, ', ') AS \"–∞–Ω–∞–ª–æ–≥–∏\"")
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

        query = f"""
        SELECT
            {', '.join(select_parts)}
        FROM parts_data p
        LEFT JOIN cross_references cr ON p.artikul_norm = cr.artikul_norm AND p.brand_norm = cr.brand_norm
        LEFT JOIN oe_data od ON cr.oe_number_norm = od.oe_number_norm
        LEFT JOIN cross_references cr2 ON cr.oe_number_norm = cr2.oe_number_norm
        GROUP BY p.artikul, p.brand, od.name, od.applicability, p.multiplicity, p.barcode, p.image_url
        """

        return query

    def export_to_csv_optimized(self, output_path: str, selected_columns=None, include_prices=True, apply_markup=True):
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
        query = self.build_export_query(selected_columns, include_prices, apply_markup)
        df = self.conn.execute(query).pl()

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º
        for exclude_word in self.exclusion_rules:
            df = df.filter(~pl.col('–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ').str.contains(exclude_word))
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞—Ü–µ–Ω–∫–∏
        if include_prices and apply_markup:
            df = df.with_columns(
                pl.when(pl.col('–ë—Ä–µ–Ω–¥').is_not_null())
                .then(
                    pl.col('–¶–µ–Ω–∞').apply(lambda price, brand: self.apply_markups(price, brand))
                )
                .otherwise(pl.col('–¶–µ–Ω–∞'))
                .alias('–¶–µ–Ω–∞ —Å –Ω–∞—Ü–µ–Ω–∫–æ–π')
            )
        df.write_csv(output_path, separator=";", include_header=True)
        st.success(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {output_path}")

    def apply_markups(self, price, brand):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞—Ü–µ–Ω–∫–∏"""
        markup = self.price_rules['brand_markups'].get(brand, self.price_rules['global_markup'])
        return price * (1 + markup)

    def show_ui_for_new_columns(self):
        """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –¥–∞–Ω–Ω—ã—Ö"""
        st.markdown("## üîß –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–ª–æ–Ω–æ–∫")
        table = st.selectbox("–¢–∞–±–ª–∏—Ü–∞", list(self.table_columns.keys()))
        col_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞")
        col_type = st.selectbox("–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö", ["VARCHAR", "DOUBLE", "INTEGER"])
        if st.button("–î–æ–±–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É"):
            if col_name:
                self.add_new_column(table, col_name, col_type)
                st.success(f"–î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü {col_name} –≤ —Ç–∞–±–ª–∏—Ü—É {table}")

        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è", type=["xlsx"])
        if uploaded_file:
            df = self.read_and_prepare_file(uploaded_file, 'custom')
            if not df.is_empty():
                self.update_table_structure_with_df(table, df)
                self.upsert_data(table, df, pk=[])  # –ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –∫–ª—é—á–∏ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã")

    def show_interface(self):
        """–ì–ª–∞–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
        st.title("üöó AutoParts Catalog")
        menu = st.sidebar.radio("–ú–µ–Ω—é", ["–ó–∞–≥—Ä—É–∑–∫–∞", "–≠–∫—Å–ø–æ—Ä—Ç", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–ù–∞—Å—Ç—Ä–æ–π–∫–∏", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"])
        if menu == "–ó–∞–≥—Ä—É–∑–∫–∞":
            self.show_upload_ui()
        elif menu == "–≠–∫—Å–ø–æ—Ä—Ç":
            self.show_export_ui()
        elif menu == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
            self.show_statistics()
        elif menu == "–ù–∞—Å—Ç—Ä–æ–π–∫–∏":
            self.show_settings()
        elif menu == "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π":
            self.show_ui_for_new_columns()

    def show_upload_ui(self):
        """UI –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤"""
        uploaded_files = {}
        for label, key in [("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (OE)", "oe"),
                           ("–ö—Ä–æ—Å—Å—ã", "cross"),
                           ("–®—Ç—Ä–∏—Ö-–∫–æ–¥—ã", "barcode"),
                           ("–í–µ—Å–æ–≥–∞–±–∞—Ä–∏—Ç—ã", "dimensions"),
                           ("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "images"),
                           ("–¶–µ–Ω—ã", "prices")]:
            uploaded_files[key] = st.file_uploader(label, type=["xlsx"])
        if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã"):
            for key, file in uploaded_files.items():
                if file:
                    path = self.data_dir / f"upload_{key}_{int(time.time())}.xlsx"
                    with open(path, "wb") as f:
                        f.write(file.getbuffer())
                    df = self.read_and_prepare_file(str(path), key)
                    if not df.is_empty():
                        table_name = self.get_table_name_by_type(key)
                        if table_name:
                            self.update_table_structure_with_df(table_name, df)
                            self.upsert_data(table_name, df, pk=[])
            st.success("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

    def show_export_ui(self):
        """UI –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        self.show_export_interface()

    def show_statistics(self):
        """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        total_parts = self.conn.execute("SELECT COUNT(*) FROM parts_data").fetchone()[0]
        st.metric("–í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤", total_parts)

    def show_settings(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∏"""
        st.markdown("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ ‚Äî –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ")

# –í –æ—Å–Ω–æ–≤–Ω–æ–º –∑–∞–ø—É—Å–∫–µ
def main():
    st.title("üöó AutoParts Catalog")
    catalog = HighVolumeAutoPartsCatalog()
    catalog.show_interface()

if __name__ == "__main__":
    main()
